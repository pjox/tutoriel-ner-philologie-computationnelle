{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "277b344c-d95d-4e7e-8fd1-aaca5d80ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-01 23:11:34,522 Reading data from ftb\n",
      "2021-12-01 23:11:34,523 Train: ftb/ftb6_train.conll\n",
      "2021-12-01 23:11:34,523 Dev: ftb/ftb6_dev.conll\n",
      "2021-12-01 23:11:34,523 Test: ftb/ftb6_test.conll\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "# define columns\n",
    "columns = {0: 'text', 2: 'pos', 3: 'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = 'ftb/'\n",
    "\n",
    "# init a corpus using column format, data folder and the names of the train, dev and test files\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='ftb6_train.conll',\n",
    "                              test_file='ftb6_test.conll',\n",
    "                              dev_file='ftb6_dev.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403ef4dc-c72f-4dc6-8bee-09259e615b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9881"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9469cf1-f2b1-4871-a11a-553a7d98d4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certes , rien ne dit qu' une seconde motion de censure sur son projet de loi , reprenant l' accord du 10 avril , n' aurait pas été la bonne mais cette probabilité , reconnaissent les socialistes , n' était pas la plus plausible .\n"
     ]
    }
   ],
   "source": [
    "print(corpus.train[0].to_tagged_string('ner'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de009a-dbd5-4198-ac8e-f3040845a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-01 23:14:05,277 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9881/9881 [00:00<00:00, 17116.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-01 23:14:05,857 Corpus contains the labels: pos (#278083), ner (#278083)\n",
      "2021-12-01 23:14:05,857 Created (for label 'ner') Dictionary with 16 tags: <unk>, O, B-Organization, I-Organization, B-Person, I-Person, B-Location, B-Company, I-Company, B-FictionCharacter, B-Product, I-Location, B-POI, I-POI, I-Product, I-FictionCharacter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 16 tags: <unk>, O, B-Organization, I-Organization, B-Person, I-Person, B-Location, B-Company, I-Company, B-FictionCharacter, B-Product, I-Location, B-POI, I-POI, I-Product, I-FictionCharacter\n",
      "2021-12-01 23:14:10,427 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:14:10,429 Model: \"SequenceTagger(\n",
      "  (embeddings): TransformerWordEmbeddings(\n",
      "    (model): CamembertModel(\n",
      "      (embeddings): RobertaEmbeddings(\n",
      "        (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
      "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "        (token_type_embeddings): Embedding(1, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): RobertaEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): RobertaLayer(\n",
      "            (attention): RobertaAttention(\n",
      "              (self): RobertaSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): RobertaSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): RobertaIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): RobertaOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): RobertaPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (linear): Linear(in_features=768, out_features=16, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2021-12-01 23:14:10,430 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:14:10,430 Corpus: \"Corpus: 9881 train + 1235 dev + 1235 test sentences\"\n",
      "2021-12-01 23:14:10,430 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:14:10,431 Parameters:\n",
      "2021-12-01 23:14:10,431  - learning_rate: \"5e-06\"\n",
      "2021-12-01 23:14:10,432  - mini_batch_size: \"4\"\n",
      "2021-12-01 23:14:10,433  - patience: \"3\"\n",
      "2021-12-01 23:14:10,433  - anneal_factor: \"0.5\"\n",
      "2021-12-01 23:14:10,434  - max_epochs: \"10\"\n",
      "2021-12-01 23:14:10,434  - shuffle: \"True\"\n",
      "2021-12-01 23:14:10,434  - train_with_dev: \"False\"\n",
      "2021-12-01 23:14:10,435  - batch_growth_annealing: \"False\"\n",
      "2021-12-01 23:14:10,435 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:14:10,436 Model training base path: \"resources/taggers/ner-transformer-ftb\"\n",
      "2021-12-01 23:14:10,436 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:14:10,436 Device: cuda:0\n",
      "2021-12-01 23:14:10,438 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:14:10,438 Embeddings storage mode: none\n",
      "2021-12-01 23:14:10,444 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:14:45,223 epoch 1 - iter 247/2471 - loss 2.81906419 - samples/sec: 28.42 - lr: 0.000000\n",
      "2021-12-01 23:15:18,203 epoch 1 - iter 494/2471 - loss 2.70303713 - samples/sec: 29.97 - lr: 0.000001\n",
      "2021-12-01 23:15:51,011 epoch 1 - iter 741/2471 - loss 2.43203794 - samples/sec: 30.13 - lr: 0.000001\n",
      "2021-12-01 23:16:26,285 epoch 1 - iter 988/2471 - loss 2.15010593 - samples/sec: 28.02 - lr: 0.000002\n",
      "2021-12-01 23:16:59,713 epoch 1 - iter 1235/2471 - loss 1.93896755 - samples/sec: 29.57 - lr: 0.000002\n",
      "2021-12-01 23:17:34,004 epoch 1 - iter 1482/2471 - loss 1.78345380 - samples/sec: 28.82 - lr: 0.000003\n",
      "2021-12-01 23:18:07,523 epoch 1 - iter 1729/2471 - loss 1.67607662 - samples/sec: 29.49 - lr: 0.000003\n",
      "2021-12-01 23:18:41,079 epoch 1 - iter 1976/2471 - loss 1.57839597 - samples/sec: 29.45 - lr: 0.000004\n",
      "2021-12-01 23:19:14,802 epoch 1 - iter 2223/2471 - loss 1.49225152 - samples/sec: 29.31 - lr: 0.000004\n",
      "2021-12-01 23:19:49,210 epoch 1 - iter 2470/2471 - loss 1.41732721 - samples/sec: 28.72 - lr: 0.000005\n",
      "2021-12-01 23:19:49,280 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:19:49,281 EPOCH 1 done: loss 1.4171 - lr 0.0000050\n",
      "2021-12-01 23:20:15,963 DEV : loss 0.5612743496894836 - f1-score (micro avg)  0.0\n",
      "2021-12-01 23:20:15,976 BAD EPOCHS (no improvement): 4\n",
      "2021-12-01 23:20:15,989 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:20:51,029 epoch 2 - iter 247/2471 - loss 0.66574577 - samples/sec: 28.21 - lr: 0.000005\n",
      "2021-12-01 23:21:25,506 epoch 2 - iter 494/2471 - loss 0.62137728 - samples/sec: 28.67 - lr: 0.000005\n",
      "2021-12-01 23:22:00,075 epoch 2 - iter 741/2471 - loss 0.59253809 - samples/sec: 28.59 - lr: 0.000005\n",
      "2021-12-01 23:22:34,211 epoch 2 - iter 988/2471 - loss 0.57083258 - samples/sec: 28.95 - lr: 0.000005\n",
      "2021-12-01 23:23:08,672 epoch 2 - iter 1235/2471 - loss 0.55436347 - samples/sec: 28.68 - lr: 0.000005\n",
      "2021-12-01 23:23:42,684 epoch 2 - iter 1482/2471 - loss 0.53955597 - samples/sec: 29.06 - lr: 0.000005\n",
      "2021-12-01 23:24:16,658 epoch 2 - iter 1729/2471 - loss 0.52175701 - samples/sec: 29.09 - lr: 0.000005\n",
      "2021-12-01 23:24:50,925 epoch 2 - iter 1976/2471 - loss 0.50026636 - samples/sec: 28.84 - lr: 0.000005\n",
      "2021-12-01 23:25:25,393 epoch 2 - iter 2223/2471 - loss 0.48121029 - samples/sec: 28.67 - lr: 0.000005\n",
      "2021-12-01 23:25:59,515 epoch 2 - iter 2470/2471 - loss 0.46269628 - samples/sec: 28.97 - lr: 0.000004\n",
      "2021-12-01 23:25:59,572 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:25:59,572 EPOCH 2 done: loss 0.4627 - lr 0.0000044\n",
      "2021-12-01 23:26:27,867 DEV : loss 0.16699707508087158 - f1-score (micro avg)  0.5651\n",
      "2021-12-01 23:26:27,880 BAD EPOCHS (no improvement): 4\n",
      "2021-12-01 23:26:27,882 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:27:01,598 epoch 3 - iter 247/2471 - loss 0.28278692 - samples/sec: 29.31 - lr: 0.000004\n",
      "2021-12-01 23:27:34,861 epoch 3 - iter 494/2471 - loss 0.27622321 - samples/sec: 29.71 - lr: 0.000004\n",
      "2021-12-01 23:28:08,657 epoch 3 - iter 741/2471 - loss 0.26980931 - samples/sec: 29.25 - lr: 0.000004\n",
      "2021-12-01 23:28:41,532 epoch 3 - iter 988/2471 - loss 0.26346105 - samples/sec: 30.07 - lr: 0.000004\n",
      "2021-12-01 23:29:15,690 epoch 3 - iter 1235/2471 - loss 0.25691616 - samples/sec: 28.94 - lr: 0.000004\n",
      "2021-12-01 23:29:49,360 epoch 3 - iter 1482/2471 - loss 0.25234444 - samples/sec: 29.36 - lr: 0.000004\n",
      "2021-12-01 23:30:23,601 epoch 3 - iter 1729/2471 - loss 0.24831661 - samples/sec: 28.87 - lr: 0.000004\n",
      "2021-12-01 23:30:57,768 epoch 3 - iter 1976/2471 - loss 0.24465699 - samples/sec: 28.93 - lr: 0.000004\n",
      "2021-12-01 23:31:32,064 epoch 3 - iter 2223/2471 - loss 0.24311286 - samples/sec: 28.82 - lr: 0.000004\n",
      "2021-12-01 23:32:06,627 epoch 3 - iter 2470/2471 - loss 0.24056073 - samples/sec: 28.60 - lr: 0.000004\n",
      "2021-12-01 23:32:06,683 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:32:06,683 EPOCH 3 done: loss 0.2406 - lr 0.0000039\n",
      "2021-12-01 23:32:35,056 DEV : loss 0.08579542487859726 - f1-score (micro avg)  0.808\n",
      "2021-12-01 23:32:35,070 BAD EPOCHS (no improvement): 4\n",
      "2021-12-01 23:32:35,071 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:33:08,842 epoch 4 - iter 247/2471 - loss 0.21251377 - samples/sec: 29.27 - lr: 0.000004\n",
      "2021-12-01 23:33:43,068 epoch 4 - iter 494/2471 - loss 0.20939316 - samples/sec: 28.88 - lr: 0.000004\n",
      "2021-12-01 23:34:17,027 epoch 4 - iter 741/2471 - loss 0.21303254 - samples/sec: 29.10 - lr: 0.000004\n",
      "2021-12-01 23:34:50,979 epoch 4 - iter 988/2471 - loss 0.20702696 - samples/sec: 29.11 - lr: 0.000004\n",
      "2021-12-01 23:35:25,276 epoch 4 - iter 1235/2471 - loss 0.20783048 - samples/sec: 28.82 - lr: 0.000004\n",
      "2021-12-01 23:36:00,021 epoch 4 - iter 1482/2471 - loss 0.20728547 - samples/sec: 28.45 - lr: 0.000004\n",
      "2021-12-01 23:36:33,384 epoch 4 - iter 1729/2471 - loss 0.20494200 - samples/sec: 29.63 - lr: 0.000004\n",
      "2021-12-01 23:37:07,341 epoch 4 - iter 1976/2471 - loss 0.20379790 - samples/sec: 29.11 - lr: 0.000003\n",
      "2021-12-01 23:37:41,355 epoch 4 - iter 2223/2471 - loss 0.20165626 - samples/sec: 29.06 - lr: 0.000003\n",
      "2021-12-01 23:38:15,216 epoch 4 - iter 2470/2471 - loss 0.20056388 - samples/sec: 29.19 - lr: 0.000003\n",
      "2021-12-01 23:38:15,276 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:38:15,276 EPOCH 4 done: loss 0.2005 - lr 0.0000033\n",
      "2021-12-01 23:38:43,598 DEV : loss 0.0674453154206276 - f1-score (micro avg)  0.8463\n",
      "2021-12-01 23:38:43,613 BAD EPOCHS (no improvement): 4\n",
      "2021-12-01 23:38:43,615 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:39:18,336 epoch 5 - iter 247/2471 - loss 0.18911535 - samples/sec: 28.47 - lr: 0.000003\n",
      "2021-12-01 23:39:52,168 epoch 5 - iter 494/2471 - loss 0.18591558 - samples/sec: 29.21 - lr: 0.000003\n",
      "2021-12-01 23:40:25,932 epoch 5 - iter 741/2471 - loss 0.18360446 - samples/sec: 29.27 - lr: 0.000003\n",
      "2021-12-01 23:40:57,568 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:40:57,569 Exiting from training early.\n",
      "2021-12-01 23:40:57,570 Saving model ...\n",
      "2021-12-01 23:40:58,237 Done.\n",
      "2021-12-01 23:40:58,238 ----------------------------------------------------------------------------------------------------\n",
      "2021-12-01 23:40:58,240 Testing using last state of model ...\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import TransformerWordEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 2. what label do we want to predict?\n",
    "label_type = 'ner'\n",
    "\n",
    "# 3. make the label dictionary from the corpus\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "print(label_dict)\n",
    "\n",
    "# 4. initialize fine-tuneable transformer embeddings WITH document context\n",
    "embeddings = TransformerWordEmbeddings(model='camembert-base',\n",
    "                                       layers=\"-1\",\n",
    "                                       subtoken_pooling=\"mean\",\n",
    "                                       fine_tune=True,\n",
    "                                       use_context=True,\n",
    "                                       )\n",
    "\n",
    "# 5. initialize bare-bones sequence tagger (no CRF, no RNN, no reprojection)\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=label_dict,\n",
    "                        tag_type='ner',\n",
    "                        use_crf=False,\n",
    "                        use_rnn=False,\n",
    "                        reproject_embeddings=False,\n",
    "                        )\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "# 7. run fine-tuning\n",
    "trainer.fine_tune('resources/taggers/ner-transformer-ftb',\n",
    "                  learning_rate=5.0e-6,\n",
    "                  mini_batch_size=4,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f199559-ef8f-4d80-98c8-01bfc6be7ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
